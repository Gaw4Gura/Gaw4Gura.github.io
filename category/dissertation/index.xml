<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dissertation | Academic</title>
    <link>https://Gaw4Gura.github.io/category/dissertation/</link>
      <atom:link href="https://Gaw4Gura.github.io/category/dissertation/index.xml" rel="self" type="application/rss+xml" />
    <description>dissertation</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 07 Jan 2022 19:33:01 +0800</lastBuildDate>
    <image>
      <url>https://Gaw4Gura.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>dissertation</title>
      <link>https://Gaw4Gura.github.io/category/dissertation/</link>
    </image>
    
    <item>
      <title>位移检测与振动测量</title>
      <link>https://Gaw4Gura.github.io/post/literature_review_2/</link>
      <pubDate>Fri, 07 Jan 2022 19:33:01 +0800</pubDate>
      <guid>https://Gaw4Gura.github.io/post/literature_review_2/</guid>
      <description>&lt;h2 id=&#34;任务分析&#34;&gt;任务分析&lt;/h2&gt;
&lt;p&gt;对于该毕业设计的具体任务，暂时做出如下分析：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对待检测目标（以一截钢管代替钢梁）能够实现实时的目标检测与位移测量&lt;/li&gt;
&lt;li&gt;考虑到实际应用背景中待检测物体振动频率较快，因此为了准确描绘物体运动，这里有两个要求：
&lt;ul&gt;
&lt;li&gt;摄像机的帧率（FPS）应该大于物体振动频率的两倍，即采样定理&lt;/li&gt;
&lt;li&gt;用于检测物体的算法应该足够快速，能够满足高速摄像机的帧率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;考虑到钢管的位移尺度相对较小，因此算法的测量结果误差应该在1mm以内&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除此之外，考虑到待检测物体（钢管）并没有通用的数据集，因此需要制作自定义的数据集用以训练神经网络，在这个任务中，大约需要200张左右的图片。&lt;/p&gt;
&lt;h2 id=&#34;目标检测yolo-mobilenet-以及swin-transformer&#34;&gt;目标检测：YOLO, MobileNet, 以及Swin Transformer&lt;/h2&gt;
&lt;h3 id=&#34;yolo-v3与yolo-tiny&#34;&gt;YOLO v3与YOLO tiny&lt;/h3&gt;
&lt;p&gt;YOLO v3是目前主流的基于深度学习的目标检测算法之一，其网络结构如下：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_1.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

相较于原先的工作采用的Darknet-19，Darknet-53在精准度上有较大的提升；而相较于采用ResNet结构的工作，Darknet-53又在效率上更胜一筹。下图展示了Darknet-53与其他网络结构的对比：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_2.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

可以看到YOLO v3每秒可以处理78张图片，而Darknet-19每秒可以处理171张图片。下图为YOLO v3网络框图：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_3.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

YOLO v3通过改变卷积的步长来控制卷积层输出的大小，并使用大尺寸特征图检测小尺寸对象和小尺寸特征图检测大尺寸对象的思想，输出为$N\times N\times [3\times (4+1+80)]$，其中N表示输出图像的大小，为$8\times 8$，有三个目标框（anchor框），每个anchor框有四个参数：中心位置$x,y$以及宽高$w,h$，除此之外，还有一个置信度和80维的类别向量。另外，YOLO v3还采用了插值上采样和concat（ResNet）等技巧，实现了良好的性能。
在训练部分，YOLO v3的训练方式如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;预测框分为三种：正确、错误、忽略&lt;/li&gt;
&lt;li&gt;任取一个标注为ground truth的数据，与生成的预测框计算Intersection over Union (IoU), IoU最大的预测框即为正确，并且一个预测框只能分配给一个标注为ground truth的数据，类别标签为1，其余为0，置信度为1，产生置信度loss、检测框loss和类别loss&lt;/li&gt;
&lt;li&gt;除正确的预测框外，与任意一个标注为ground truth的数据的IoU大于0.5的为忽略检测框，不产生loss&lt;/li&gt;
&lt;li&gt;除正确的预测框外，与所有标注为ground truth的数据的IoU都小于0.5的为错误检测框，置信度为0，产生置信度loss&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，YOLO v3的loss函数为一般的交叉熵函数，而优化器可以使用随机梯度下降优化器或是Adam优化器。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_4.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

上图为YOLO v3 tiny的网络结构，相比于YOLO v3去除了一些特征层，并且只保留两个独立的预测分支，在工程上更为常用，速度也比YOLO v3更快（在1660Ti上实现了100FPS）。&lt;/p&gt;
&lt;h3 id=&#34;mobilenet-v2&#34;&gt;MobileNet v2&lt;/h3&gt;
&lt;p&gt;MobileNet v2在v1的基础上进行改进，同样基于深度可分离卷积。深度可分离卷积在Inception卷积的基础上进行构造，将输入进行$1\times 1$的卷积之后分成若干个$3\times 3$的卷积，并在最后进行concat.下图展示了深度可分离卷积Xception的结构：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_5.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

MobileNet v2提出了两个新观点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Bottlenecks，用于解决ReLU激活函数$\max{x, 0}$造成的非线性问题&lt;/li&gt;
&lt;li&gt;Inverted residuals，用于提升梯度在乘积层之间的传播能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图为MobileNet v2的网络结构：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_6.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

MobileNet v2仅用CPU就可以实现80ms单张图片的速度。&lt;/p&gt;
&lt;h3 id=&#34;swin-transformer&#34;&gt;Swin Transformer&lt;/h3&gt;
&lt;p&gt;swin transformer是一种通用的用于图像处理的transformer结构，在ImageNet-22k上达到了775.2 image/s的吞吐量，并摘得ImageNet多个项目的statement of the art (SOTA).下图为swin transformer的结构：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_7.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

将一张图片分割为少量的window，并在每一个window中做transformer的attention操作，并利用一些技巧提升感受野，达到超过ResNet的效果。目标检测也是swin transformer的经典应用之一。
swin transformer也有显著的缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练轮数要求高，收敛慢&lt;/li&gt;
&lt;li&gt;对于不同输入尺寸的图片，需要定制transformer结构&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图为vision transformer (ViT)的结构：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_11.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

将图像分割成$P\times P$的window，使用全连接层进行patch embedding，加入一维向量表示每个window原本的位置，之后使用multi-head self-attention机制作为网络的主要结构。
与传统的attention机制相比，multi-head self-attention使用若干不同的变换参数，并进行concat操作。
$$\text{attention}(Q,V,K)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$$
$$head_{i}=\text{attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$$
$$\text{multi-head}(Q,K,V)=\text{concat}(head_{1}, head_{2}, \dots, head_{h})W^{O}$$&lt;/p&gt;
&lt;h2 id=&#34;一种基于cnn的适用于高速摄像机情景的目标检测方案&#34;&gt;一种基于CNN的适用于高速摄像机情景的目标检测方案&lt;/h2&gt;
&lt;p&gt;针对例如1000FPS的高速相机，上述几种神经网络的效率都不足以达到实时检测要求。高速成像的摄像机几乎不会错过待检测物体的运动，具有高时间密度和低延迟等优势，而用于此类场景的神经网络也很难具有较高的计算复杂度和网络深度。[*]中提出了一种基于数据清洗和数据集成的用于识别高速摄像机采集的图像中的对象的神经网络，并且给出了一个训练数据集。下图为该工作的框架：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_8.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

其中，数据清洗过程旨在去除尽可能多的错误的region of interest (ROI)区域，以减少后续进行目标识别的工作量。文章中提到的算法为根据预训练网络的输出准备一组ROI图像，并使用支持向量机等方法得到一个评分函数，使用这个评分函数来取舍待识别图像的ROI区域，评估的标准包括了像素值的统计量、像素值的空间导数（拉普拉斯算子等）、物体位置的时间导数（加速度等）等。在目标识别的部分，作者将多个ROI输入以提高检测精度，并通过减少现有的目标识别网络的特征层以牺牲网络的泛化能力为代价提高了网络针对某一特定任务的检测速度。最终，该工作满足了1000FPS的高速相机的应用要求。以下两图为实验中该网络结构的表现：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_9.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./fig_10.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;对于传统的CNN网络结构，为了实现高速实时检测物体运动的目标，需要仔细设计数据集以及输入图像处理的手段，减少神经网络的工作量，并且减少CNN卷积层的数量以提高效率（对于一种特定的任务，这样做对识别精度的影响并不大）。此外，swin transformer也是一大可以探究的应用于高速相机图像实时目标识别的选择。&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.02767&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YOLO v3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/76802514&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YOLO v3解读&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/93809416&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YOLO v3及YOLO v3 tiny&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/137971709&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YOLO v3 tiny代码及运行效率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.04381&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MobileNet v2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/98001811&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;深度可分离卷积&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/u011974639/article/details/79199588&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MobileNet v2解读&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.14030&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;swin transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/qq_41111734/article/details/116353615&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;swin transformer解读&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/amusi1994/article/details/116113351&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;swin transformer实现目标检测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_44106928/article/details/110268312&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vision transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/qq_38343151/article/details/102993202&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;multi-head self-attention&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[*] S. Namiki, K. Yokoyama, S. Yachida, T. Shibata, H. Miyano and M. Ishikawa, &amp;ldquo;Online Object Recognition Using CNN-based Algorithm on High-speed Camera Imaging: Framework for fast and robust high-speed camera object recognition based on population data cleansing and data ensemble,&amp;rdquo; 2020 25th International Conference on Pattern Recognition (ICPR), 2021, pp. 2025-2032, doi: 10.1109/ICPR48806.2021.9413042.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>位移检测与振动测量</title>
      <link>https://Gaw4Gura.github.io/post/literature_review_1/</link>
      <pubDate>Sat, 01 Jan 2022 23:16:56 +0800</pubDate>
      <guid>https://Gaw4Gura.github.io/post/literature_review_1/</guid>
      <description>&lt;h2 id=&#34;传统的基于视觉的振动测量方法&#34;&gt;传统的基于视觉的振动测量方法&lt;/h2&gt;
&lt;p&gt;在结构安全监测领域，振动测量在预测潜在灾害和对设计维护活动的结构参数估计方面都起着至关重要的作用。大量的监测技术可用于结构振动监测，而应用最广泛的可能是基于加速度计的测量和形变估计。虽然这些技术适用于大多数应用，但它们在特定应用中存在一些限制，例如在钢梁这样细长的结构中，非接触式测量技术将是首选。这样可以避免负载效应，并可以使得振动的测量在许多方位同时进行。基于视觉的振动测量有许多好处，例如可以在不使用插值的情况下测量振动最大或是最小的点，并且只要结构仍然落在相机镜头内也不需要改变试验设备的位置 [1].&lt;/p&gt;
&lt;p&gt;[1]中介绍了一种由单个传感器（相机）进行监控的基于视觉的细长结构阻尼振动的测量方案。该工作使用边缘检测的方法来识别电缆结构，其中，“边缘”在灰度图像中可以被定义为灰度突变的像素点的集合。考虑到待测量振动的物体通常是一体的，该方法可以被推广到更一般的物体的振动测量中。文章中提到的方法在实践里有两个要求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;摄像机应该与待测物体平行&lt;/li&gt;
&lt;li&gt;摄像机的采样频率应该至少为电缆阻尼振动频率的两倍，即采样定理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;采集到图像以后，系统通过五个步骤来得到电缆阻尼振动的估计：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;电缆的边缘像素由每一帧图像进行边缘检测获得&lt;/li&gt;
&lt;li&gt;通过对像素坐标进行线性拟合来消除数据中的噪声以及量化误差的影响&lt;/li&gt;
&lt;li&gt;迭代以上两步，直到电缆边缘所有像素的坐标&lt;/li&gt;
&lt;li&gt;在时域中重建每个点的垂直位移&lt;/li&gt;
&lt;li&gt;对信号进行傅里叶变换，并在频域中估计每个点的水平位移&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;图1展示了这个系统的工作原理。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./5341309-fig-2-source-small.gif&#34; alt=&#34;图1 [1]. (a)对电缆的边缘检测。(b)对边缘像素进行线性拟合。(c)阻尼运动振幅的估计&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;[2]介绍了基于多台摄像头和一台笔记本电脑的桥梁结构监测系统。通过摄像头捕捉桥梁结构的边缘和铆钉等标志物，并且通过一种被称为方向码匹配（Orientation Code Matching, OCM）的算法进行视频处理。该算法基于匹配梯度信息，围绕每个像素以方向码的形式计算，而不是直接计算灰度级来进行边缘检测。对于像素$I(x,y)$，其水平和垂直方向的方向导数分别为$\nabla I_{x}=\frac{\partial I}{\partial x},\nabla I_{y}=\frac{\partial I}{\partial y}$.使用$\tan^{-1}(\nabla I_{y}/\nabla I_{x})$定义$I(x,y)$的方位角，通过灰度阈值$\Gamma$，可以得到方向角的高斯表示。随后，可以利用测量函数对方向码进行匹配和测量。在使用振动的桌子对系统进行测试的时候，预先设计的黑白目标面板（216 mm × 280 mm）固定在电磁振动台上，传统方法安装在振动台和固定参考点之间。在距振动台 5.5 米的静止位置放置一个带有 75 毫米镜头的摄像机。相机以每秒 60 帧的速度捕捉目标面板，并通过在连接的计算机上执行 OCM 技术和像素级分析来检测目标面板的位移。在测量之前，使用预先设计的目标面板测量数字图像的实际像素大小。像素的尺寸校准为 0.54 毫米/像素，测量误差的理论值为 0.27 毫米 [2]。在实际应用中，还应该考虑自然环境对系统的影响。&lt;/p&gt;
&lt;p&gt;[3]利用振动学的基础知识（杨氏模量等）和机器视觉检测物体的细微运动，并从中推断物体的材料特性。[4] (1993)则介绍了递归神经网络（RNN）在检测轴承运动中的应用。[5]提出了一种基于多摄像头系统的多散斑匹配（multi speckle matching）方法，采用该方法计算不同相机坐标的旋转矩阵和平移矩阵。 将所有相机的测量点转换为相同的坐标系并构建 3D 形状。 通过减去平移前后的坐标得到全场位移。 使用多相机系统进行铝板测量。结果表明位移场转换误差小于0.1mm。&lt;/p&gt;
&lt;h2 id=&#34;新型基于深度学习的位移与振动测量方法&#34;&gt;新型基于深度学习的位移与振动测量方法&lt;/h2&gt;
&lt;p&gt;[6]介绍了一个基于YOLO v3$^{*}$的社交距离测量方案。该方案以摄像头的视频帧为输入，采用基于YOLO v3算法的开源物体检测预训练模型进行行人检测。随后，视频帧被转换为自上而下的视图，用于与 2D平面的距离测量，以估计人与人之间的距离。图2展示了这个算法的流程图：&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./9243478-fig-2-source-small.gif&#34; alt=&#34;图2 [6]. 基于YOLO v3进行社交距离测量算法流程图&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;利用经过COCO数据集（含有人类以及行人类别）训练的YOLO v3网络对行人进行目标识别，得到行人的&lt;strong&gt;bounding_box&lt;/strong&gt;的中心坐标$(x,y)$，宽高$(w,h)$，目标置信度以及类别标签。尽管YOLO v3可以进行多种目标的检测，但是在本文的工作中，系统将忽略其余目标物，只关注行人类别的对象。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./9243478-fig-4-source-small.gif&#34; alt=&#34;图3 [6]. 将透视图转化为俯视图&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;街道图像的专注区域（region of interest, ROI）被转换为包含480×480像素的自上而下的2D视图，如图3所示。通过应用相机视图校准，计算透视图变成自上而下的视图。在OpenCV中，透视变换是一种简单的相机校准方法，它涉及在透视图中选择四个点并将它们映射到2D图像视图中的矩形的角上。 假设每个人都站在同一水平平面上。行人之间的实际距离对应于自顶向下视图中的像素数可以估计。最后，利用两个行人的&lt;strong&gt;bounding_box&lt;/strong&gt;中心的距离$d=\sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}$以及相机的缩放倍数可以计算得到行人之间的社交距离。&lt;/p&gt;
&lt;p&gt;[7]则介绍了基于Mask R-CNN$^{*}$的在实验室实验中自动跟踪和测量结构试件位移和振动的系统。Mask R-CNN可以从固定摄像机录制的视频中定位目标并监控其运动。 为了提高精度和去除噪声，包括了尺度不变特征变换（Scale-invariant Feature Transform, SIFT）和各种用于信号处理的滤波器等技术。作者利用三个小型钢筋混凝土梁的试验和振动台试验来验证所提出的方法。结果表明，所提出的深度学习方法可以实现在实验室实验中自动精确测量被测结构构件的运动的目标。图4为这种Mask R-CNN的结构：&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./bai1-p4-bai-small.gif&#34; alt=&#34;图4 [7]. 用于追踪物体位移和振动的Mask R-CNN网络结构。&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;图5展示了该算法的流程图：&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./bai3-p4-bai-small.gif&#34; alt=&#34;图5 [7]. 利用Mask R-CNN对钢筋混凝土梁位移与振动进行追踪的算法流程图。&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;不妨令$d_{u}^{j},d_{v}^{j}$表示第一帧物体的&lt;strong&gt;bounding_box&lt;/strong&gt;和第$j$帧物体的&lt;strong&gt;bounding_box&lt;/strong&gt;之间水平以及竖直方向上的位移分量，$s$表示从像素转化为长度单位的比例，则物体的实际位移可以表示为$d_{x}^{j}=s\cdot d_{u}^{j}$以及$d_{y}^{j}=s\cdot d_{v}^{j}$.其中，当&lt;strong&gt;bounding_box&lt;/strong&gt;不能很好地拟合物体的实际边框时，作者利用SIFT算法将&lt;strong&gt;bounding_box&lt;/strong&gt;重新与物体进行匹配。图6阐明了该算法的工作原理。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./bai2-p4-bai-large.gif&#34; alt=&#34;图6 [7]. 对&amp;lt;strong&amp;gt;bounding_box&amp;lt;/strong&amp;gt;进行SIFT算法重新匹配。&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;$^{*}$：关于YOLO v3等用于实现目标识别或边缘检测的神经网络结构将在之后的文章中学习与介绍。&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;根据文献[1 &amp;ndash; 7]，基于视觉的振动和位移测量具有广阔的应用空间，并在表现上优于传统方法。而根据[6], [7]，基于深度学习的方法又在基于传统计算机视觉的方法上更进一步。除了上述文献，图像处理技术（image processing technique）[8]，上采样互相关技术（up-sampled cross correlation）[9]，自适应关注区域算法（adaptive Region of Interest algorithm）[10]，改进的泰勒近似（modified Taylor approximation）[11]，使用加速鲁棒特征（Speeded-Up Robust Features, SURF）的轮廓提取[12]等技术[13 &amp;ndash; 20]也需要进一步的学习与研究。&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] G. Busca, A. Cigada, A. Manenti and E. Zappa, &amp;ldquo;Vision-based measurements for slender structures vibration monitoring,&amp;rdquo; 2009 IEEE Workshop on Environmental, Energy, and Structural Monitoring Systems, 2009, pp. 98-102, doi: 10.1109/EESMS.2009.5341309.&lt;/p&gt;
&lt;p&gt;[2] Y. Fukuda, M. Q. Feng, Y. Narita, S. Kaneko and T. Tanaka, &amp;ldquo;Vision-Based Displacement Sensor for Monitoring Dynamic Response Using Robust Object Search Algorithm,&amp;rdquo; in IEEE Sensors Journal, vol. 13, no. 12, pp. 4725-4732, Dec. 2013, doi: 10.1109/JSEN.2013.2273309.&lt;/p&gt;
&lt;p&gt;[3] A. Davis, K. L. Bouman, J. G. Chen, M. Rubinstein, F. Durand and W. T. Freeman, &amp;ldquo;Visual vibrometry: Estimating material properties from small motions in video,&amp;rdquo; 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 5335-5343, doi: 10.1109/CVPR.2015.7299171.&lt;/p&gt;
&lt;p&gt;[4] I. E. Alguindigue, A. Loskiewicz-Buczak and R. E. Uhrig, &amp;ldquo;Monitoring and diagnosis of rolling element bearings using artificial neural networks,&amp;rdquo; in IEEE Transactions on Industrial Electronics, vol. 40, no. 2, pp. 209-217, April 1993, doi: 10.1109/41.222642.&lt;/p&gt;
&lt;p&gt;[5] T. Hu, L. Ma, D. Jiang and Q. Fei, &amp;ldquo;Multi-camera based full-field 3D displacement measurement using digital image correlation,&amp;rdquo; 2020 13th International Symposium on Computational Intelligence and Design (ISCID), 2020, pp. 164-167, doi: 10.1109/ISCID51228.2020.00043.&lt;/p&gt;
&lt;p&gt;[6] Y. C. Hou, M. Z. Baharuddin, S. Yussof and S. Dzulkifly, &amp;ldquo;Social Distancing Detection with Deep Learning Model,&amp;rdquo; 2020 8th International Conference on Information Technology and Multimedia (ICIMU), 2020, pp. 334-338, doi: 10.1109/ICIMU49871.2020.9243478.&lt;/p&gt;
&lt;p&gt;[7] Y. Bai, R. M. Abduallah, H. Sezen and A. Yilmaz, &amp;ldquo;Automatic Displacement and Vibration Measurement in Laboratory Experiments with A Deep Learning Method,&amp;rdquo; 2021 IEEE Sensors, 2021, pp. 1-4, doi: 10.1109/SENSORS47087.2021.9639455.&lt;/p&gt;
&lt;p&gt;[8] J.-J. Lee and M. Shinozuka, &amp;ldquo;Real-time displacement measurement of a flexible bridge using digital image processing techniques&amp;rdquo;, &lt;em&gt;Experimental mechanics&lt;/em&gt;, vol. 46, no. 1, pp. 105-114, 2006.&lt;/p&gt;
&lt;p&gt;[9] D. Feng, M. Q. Feng, E. Ozer and Y. Fukuda, &amp;ldquo;A vision-based sensor for noncontact structural displacement measurement&amp;rdquo;, &lt;em&gt;Sensors&lt;/em&gt;, vol. 15, no. 7, pp. 16557-16575, 2015.&lt;/p&gt;
&lt;p&gt;[10] J. Lee, K.-C. Lee, S. Cho and S.-H. Sim, &amp;ldquo;Computer vision-based structural displacement measurement robust to light-induced image degradation for in-service bridges&amp;rdquo;, &lt;em&gt;Sensors&lt;/em&gt;, vol. 17, no. 10, pp. 2317, 2017.&lt;/p&gt;
&lt;p&gt;[11] B. Liu, D. Zhang and J. Guo, &amp;ldquo;Vision-based displacement measurement sensor using modified taylor approximation approach&amp;rdquo;, &lt;em&gt;Optical Engineering&lt;/em&gt;, vol. 55, no. 11, pp. 114103, 2016.&lt;/p&gt;
&lt;p&gt;[12] Z. Yin, C. Wu and G. Chen, &amp;ldquo;Concrete crack detection through full-field displacement and curvature measurements by visual mark tracking: A proof-of-concept study&amp;rdquo;, &lt;em&gt;Structural Health Monitoring&lt;/em&gt;, vol. 13, no. 2, pp. 205-218, 2014.&lt;/p&gt;
&lt;p&gt;[13] J. Guo and C. Zhu, &amp;ldquo;Dynamic displacement measurement of largescale structures based on the lucas–kanade template tracking algorithm&amp;rdquo;, &lt;em&gt;Mechanical Systems and Signal Processing&lt;/em&gt;, vol. 66, pp. 425-436, 2016.&lt;/p&gt;
&lt;p&gt;[14] C.-Z. Dong, O. Celik and F. N. Catbas, &amp;ldquo;Marker-free monitoring of the grandstand structures and modal identification using computer vision methods&amp;rdquo;, &lt;em&gt;Structural Health Monitoring&lt;/em&gt;, vol. 18, no. 5-6, pp. 1491-1509, 2019.&lt;/p&gt;
&lt;p&gt;[15] J. Hu and P. F. Pai, &amp;ldquo;Experimental study of resonant vibrations of suspended steel cables using a 3d motion analysis system&amp;rdquo;, &lt;em&gt;Journal of Engineering Mechanics&lt;/em&gt;, vol. 138, no. 6, pp. 640-661, 2012.&lt;/p&gt;
&lt;p&gt;[16] J. G. Chen, A. Davis, N. Wadhwa, F. Durand, W. T. Freeman and O. Büyüköztürk, &amp;ldquo;Video camera–based vibration measurement for civil infrastructure applications&amp;rdquo;, &lt;em&gt;Journal of Infrastructure Systems&lt;/em&gt;, vol. 23, no. 3, pp. B4016013, 2017.&lt;/p&gt;
&lt;p&gt;[17] V. Hoskere, J.-W. Park, H. Yoon and B. F. Spencer, &amp;ldquo;Vision-based modal survey of civil infrastructure using unmanned aerial vehicles&amp;rdquo;, &lt;em&gt;Journal of Structural Engineering&lt;/em&gt;, vol. 145, no. 7, pp. 04019062, 2019.&lt;/p&gt;
&lt;p&gt;[18] C.-Z. Dong, O. Celik, F. N. Catbas, E. J. O’Brien and S. Taylor, &amp;ldquo;Structural displacement monitoring using deep learning-based full field optical flow methods&amp;rdquo;, &lt;em&gt;Structure and Infrastructure Engineering&lt;/em&gt;, vol. 16, no. 1, pp. 51-71, 2020.&lt;/p&gt;
&lt;p&gt;[19] C.-Z. Dong and F. N. Catbas, &amp;ldquo;A non-target structural displacement measurement method using advanced feature matching strategy&amp;rdquo;, &lt;em&gt;Advances in Structural Engineering&lt;/em&gt;, vol. 22, no. 16, pp. 3461-3472, 2019.&lt;/p&gt;
&lt;p&gt;[20] C.-Z. Dong, O. Celik, F. N. Catbas, E. OBrien and S. Taylor, &amp;ldquo;A robust vision-based method for displacement measurement under adverse environmental factors using spatio-temporal context learning and taylor approximation&amp;rdquo;, &lt;em&gt;Sensors&lt;/em&gt;, vol. 19, no. 14, pp. 3197, 2019.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
